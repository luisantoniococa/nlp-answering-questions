{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default', 'masked-lm', 'causal-lm', 'sequence-classification', 'token-classification', 'question-answering']\n"
     ]
    }
   ],
   "source": [
    "from transformers.onnx.features import FeaturesManager\n",
    "\n",
    "distilbert_features = list(FeaturesManager.get_supported_features_for_model_type(\"bert\").keys())\n",
    "print(distilbert_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.10.2+cpu\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'start_logits', 'end_logits'})\n",
      "\t- Validating ONNX Model output \"start_logits\":\n",
      "\t\t-[✓] (2, 8) matches (2, 8)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"end_logits\":\n",
      "\t\t-[✓] (2, 8) matches (2, 8)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "All good, model saved at: onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --model=distilbert-base-uncased-distilled-squad --feature=question-answering onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from onnxruntime import (\n",
    "    InferenceSession, SessionOptions, GraphOptimizationLevel\n",
    ")\n",
    "from transformers import (\n",
    "    TokenClassificationPipeline, AutoTokenizer, AutoModelForTokenClassification,\n",
    "    DistilBertTokenizer, DistilBertForQuestionAnswering, QuestionAnsweringPipeline,\n",
    "    AutoModelForQuestionAnswering\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = SessionOptions() # initialize session options\n",
    "options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "session = InferenceSession(\n",
    "    \"onnx/model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "# disable session.run() fallback mechanism, it prevents for a reset of the execution provider\n",
    "session.disable_fallback() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxQuestionAnsweringPipeline(QuestionAnsweringPipeline):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    \n",
    "    def _forward(self, model_inputs):\n",
    "\n",
    "        # This comes from the original implementation of the pipeline\n",
    "        special_tokens_mask = model_inputs.pop(\"special_tokens_mask\")\n",
    "        offset_mapping = model_inputs.pop(\"offset_mapping\", None)\n",
    "        sentence = model_inputs.pop(\"sentence\")\n",
    "\n",
    "        inputs = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()} # dict of numpy arrays\n",
    "        outputs_name = session.get_outputs()[0].name # get the name of the output tensor\n",
    "\n",
    "        logits = session.run(output_names=[outputs_name], input_feed=inputs)[0] # run the session\n",
    "        logits = torch.tensor(logits) # convert to torch tensor to be compatible with the original implementation\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"special_tokens_mask\": special_tokens_mask,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"sentence\": sentence,\n",
    "            **model_inputs,\n",
    "        }\n",
    "\n",
    "    # We need to override the preprocess method because the onnx model is waiting for the attention masks as inputs\n",
    "    # along with the embeddings.\n",
    "    def preprocess(self, question, context, offset_mapping=None):\n",
    "        truncation = True if self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0 else False\n",
    "        model_inputs = self.tokenizer(\n",
    "            sentence,\n",
    "            return_attention_mask=True, # This is the only difference from the original implementation\n",
    "            return_tensors=self.framework,\n",
    "            truncation=truncation,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_offsets_mapping=self.tokenizer.is_fast,\n",
    "        )\n",
    "        if offset_mapping:\n",
    "            model_inputs[\"offset_mapping\"] = offset_mapping\n",
    "\n",
    "        model_inputs[\"context\"] = context\n",
    "        model_inputs[\"question\"] = question\n",
    "\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_from_hub = \"distilbert-base-uncased-distilled-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_from_hub)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name_from_hub)\n",
    "\n",
    "onnx_pipeline = OnnxQuestionAnsweringPipeline(\n",
    "    task=\"question-answering\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"pt\",\n",
    "    aggregation_strategy=\"simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess() missing 1 required positional argument: 'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m onnx_pipeline(question\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mWhat kind of scan is this?\u001b[39;49m\u001b[39m'\u001b[39;49m, context\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNo spinal cord demyelination. Spinal MRI within normal limits.  Right cerebellar hemangioblastoma again seen Approved and ElectronicallySigned by\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\luico\\.conda\\envs\\hf-onnx\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:251\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m examples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_args_parser(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(examples) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(examples[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(examples, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luico\\.conda\\envs\\hf-onnx\\lib\\site-packages\\transformers\\pipelines\\base.py:1027\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1026\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1027\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\luico\\.conda\\envs\\hf-onnx\\lib\\site-packages\\transformers\\pipelines\\base.py:1048\u001b[0m, in \u001b[0;36mChunkPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1047\u001b[0m     all_outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1048\u001b[0m     \u001b[39mfor\u001b[39;00m model_inputs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpreprocess_params):\n\u001b[0;32m   1049\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1050\u001b[0m         all_outputs\u001b[39m.\u001b[39mappend(model_outputs)\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocess() missing 1 required positional argument: 'context'"
     ]
    }
   ],
   "source": [
    "\n",
    "result = onnx_pipeline(question='What kind of scan is this?', context='No spinal cord demyelination. Spinal MRI within normal limits.  Right cerebellar hemangioblastoma again seen Approved and ElectronicallySigned by')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "\n",
    "# Convert the model to ONNX format\n",
    "torch.onnx.export(model,\n",
    "                  (torch.zeros(1, 128).long(), torch.zeros(1, 128).long()),\n",
    "                  \"qamodel.onnx\",\n",
    "                  input_names=[\"input_ids\", \"attention_mask\"],\n",
    "                  output_names=[\"start_scores\", \"end_scores\"],\n",
    "                  dynamic_axes={\"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                                \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                                \"start_scores\": {0: \"batch_size\"},\n",
    "                                \"end_scores\": {0: \"batch_size\"}},\n",
    "                opset_version=11)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "# Load the ONNX model\n",
    "session = onnxruntime.InferenceSession(\"qamodel.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object QuestionAnsweringPipeline.preprocess at 0x0000029E16F64C80>\n",
      "[[ 101 2054 2003 2115 2171 1029  102 2026 2171 2003 2198 1012  102]]\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "{'example': <transformers.data.processors.squad.SquadExample object at 0x0000029E279836A0>, 'is_last': True, 'input_ids': tensor([[ 101, 2054, 2003, 2115, 2171, 1029,  102, 2026, 2171, 2003, 2198, 1012,\n",
      "          102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'p_mask': tensor([[False,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "         False, False,  True]]), 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]), 'cls_index': None, 'example_index': 0, 'unique_id': 0, 'paragraph_len': 0, 'token_is_max_context': 0, 'tokens': [], 'token_to_orig_map': {}, 'start_position': 0, 'end_position': 0, 'is_impossible': False, 'qas_id': None, 'encoding': Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])}\n",
      "[array([[-5.4723043 , -1.9892555 , -5.42635   , -4.5206323 , -4.5369883 ,\n",
      "        -6.4154053 , -5.529415  ,  2.9449806 , -0.88666224, -2.7187648 ,\n",
      "         6.8037086 , -3.4157152 , -5.5294    ]], dtype=float32), array([[-0.8526628, -1.9087323, -4.9586883, -4.492498 , -3.715437 ,\n",
      "        -5.1584134, -1.0872303, -2.6226451, -1.5220027, -4.6095796,\n",
      "         6.3161626,  2.7676857, -1.0871414]], dtype=float32)]\n",
      "<transformers.data.processors.squad.SquadExample object at 0x0000029E279836A0>\n",
      "[[False  True  True  True  True  True  True False False False False False\n",
      "   True]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m qa_pipeline \u001b[39m=\u001b[39m ONNXQuestionAnsweringPipeline(model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m     47\u001b[0m \u001b[39m# Use the pipeline for question answering with context\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m result \u001b[39m=\u001b[39m qa_pipeline(question\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWhat is your name?\u001b[39;49m\u001b[39m\"\u001b[39;49m, context\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMy name is John.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     49\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\luico\\.conda\\envs\\hf-onnx\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:251\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m examples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_args_parser(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(examples) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(examples[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(examples, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luico\\.conda\\envs\\hf-onnx\\lib\\site-packages\\transformers\\pipelines\\base.py:1027\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1026\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1027\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\luico\\.conda\\envs\\hf-onnx\\lib\\site-packages\\transformers\\pipelines\\base.py:1051\u001b[0m, in \u001b[0;36mChunkPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1049\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1050\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(model_outputs)\n\u001b[1;32m-> 1051\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpostprocess(all_outputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpostprocess_params)\n\u001b[0;32m   1052\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\luico\\.conda\\envs\\hf-onnx\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:437\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.postprocess\u001b[1;34m(self, model_outputs, top_k, handle_impossible_answer, max_answer_len)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     \u001b[39m# Convert the answer (tokens) back to the original text\u001b[39;00m\n\u001b[0;32m    432\u001b[0m     \u001b[39m# Score: score from the model\u001b[39;00m\n\u001b[0;32m    433\u001b[0m     \u001b[39m# Start: Index of the first character of the answer in the context string\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     \u001b[39m# End: Index of the character following the last character of the answer in the context string\u001b[39;00m\n\u001b[0;32m    435\u001b[0m     \u001b[39m# Answer: Plain text of the answer\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     question_first \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpadding_side \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 437\u001b[0m     enc \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m    439\u001b[0m     \u001b[39m# Encoding was *not* padded, input_ids *might*.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[39m# It doesn't make a difference unless we're padding on\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[39m# the left hand side, since now we have different offsets\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     \u001b[39m# everywhere.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpadding_side \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'encoding'"
     ]
    }
   ],
   "source": [
    "from transformers import QuestionAnsweringPipeline\n",
    "import numpy as np\n",
    "class ONNXQuestionAnsweringPipeline(QuestionAnsweringPipeline):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.session = session\n",
    "    def _forward(self, model_inputs):\n",
    "        # Run inference with the ONNX model\n",
    "        input_ids = model_inputs[\"input_ids\"].numpy()\n",
    "        attention_mask = model_inputs[\"attention_mask\"].numpy()\n",
    "        print(input_ids)\n",
    "        print(attention_mask)\n",
    "        print(model_inputs)\n",
    "        outputs = self.session.run(\n",
    "            None, \n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "            )\n",
    "        print(outputs)\n",
    "        start_scores, end_scores = outputs\n",
    "        # Find the best start and end positions\n",
    "        start = np.argmax(start_scores)\n",
    "        end = np.argmax(end_scores)\n",
    "\n",
    "        # Include the example in the output\n",
    "        example = model_inputs[\"example\"]\n",
    "        p_mask = model_inputs[\"p_mask\"].numpy()\n",
    "        print(example)\n",
    "        print(p_mask)\n",
    "        return {\n",
    "            \"start_logits\": start_scores, \n",
    "            \"end_logits\": end_scores, \n",
    "            \"start\": start, \n",
    "            \"end\": end,\n",
    "            \"example\":example,\n",
    "            \"p_mask\": p_mask\n",
    "        }\n",
    "\n",
    "    def preprocess(self, example):\n",
    "            model_inputs = super().preprocess(example)\n",
    "            print(model_inputs) # Add a print statement here to see the value of model_inputs\n",
    "            return model_inputs\n",
    "\n",
    "\n",
    "# Create an instance of the custom pipeline\n",
    "qa_pipeline = ONNXQuestionAnsweringPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Use the pipeline for question answering with context\n",
    "result = qa_pipeline(question=\"What is your name?\", context=\"My name is John.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<class 'bool'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
